{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "stm = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = pd.read_csv('PostLinks.csv')\n",
    "posts = pd.read_csv('Posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertices list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.97 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2245"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "u_source = links['PostId'].unique().tolist()\n",
    "u_target = links['RelatedPostId'].unique().tolist()\n",
    "\n",
    "u_source.extend(u_target)\n",
    "\n",
    "Vertices_list = list(dict.fromkeys(u_source))\n",
    "Vertices_list.sort()\n",
    "len(Vertices_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edge_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1558"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_list = []\n",
    "Edges_list = list(zip(links['PostId'],links['RelatedPostId']))\n",
    "Edges_list.sort()\n",
    "Edges_unique = list(dict.fromkeys(Edges_list))\n",
    "len(Edges_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjacency list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "V = len(Vertices_list)\n",
    "\n",
    "Adj_list = []\n",
    "\n",
    "for s in range(V):\n",
    "    Adj_list.append([])\n",
    "\n",
    "for i in Edges_unique:\n",
    "    a = Vertices_list.index(i[0])\n",
    "    Adj_list[a].append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segregating src and adjacent target nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_nodes = [] \n",
    "target_nodes = []\n",
    "\n",
    "for i in Adj_list:\n",
    "    if len(i)>1:\n",
    "        ind = Adj_list.index(i)\n",
    "        src_nodes.append(Vertices_list[ind])\n",
    "        target_nodes.append(i)\n",
    "        \n",
    "src_nodes_u = list(dict.fromkeys(src_nodes))\n",
    "\n",
    "# Hard coding to remove duplicates from target_nodes\n",
    "\n",
    "p = [66,73,74,94,119,120]\n",
    "for i in p:\n",
    "    target_nodes.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RANKING RELATED POSTS - hyp(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_posts = posts[posts.Id.isin(src_nodes)]\n",
    "k = [i for i in src_nodes if i not in list(src_posts['Id']) ]\n",
    "try:\n",
    "    for i in k:\n",
    "        target_nodes.pop(src_nodes_u.index(i))\n",
    "        src_nodes_u.pop(src_nodes_u.index(i))\n",
    "except:\n",
    "    print('no element fround')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_scrapped = [] \n",
    "\n",
    "for post in src_posts['Title']:\n",
    "    soup = BeautifulSoup(post, 'html.parser')\n",
    "\n",
    "    article = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', soup.text, flags=re.MULTILINE)\n",
    "\n",
    "    article = article.replace('\\n','')\n",
    "\n",
    "    src_scrapped.append(article)\n",
    "    \n",
    "\n",
    "    # with open('posts.txt', 'w') as file:\n",
    "    #     file.write(article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to be populated in dropdown\n",
    "# src_scrapped "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[739, 808]], [1141])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_nodes[:1],src_nodes_u[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action on submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def relAnswer(question):\n",
    "    \n",
    "    trg_scrapped = []\n",
    "    \n",
    "    sel_que = question\n",
    "    \n",
    "    q_ind = src_scrapped.index(sel_que)\n",
    "\n",
    "    # adjacent nodes of selected question \n",
    "    \n",
    "    answers_ind = target_nodes[q_ind]\n",
    "    \n",
    "    soup_q = BeautifulSoup(sel_que, 'html.parser')    \n",
    "    \n",
    "#     scrapped_que = ''\n",
    "\n",
    "    scrapped_que = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', soup_q.text, flags=re.MULTILINE)\n",
    "\n",
    "    scrapped_que = scrapped_que.replace('\\n','')\n",
    "    \n",
    "    \n",
    "    for ind in answers_ind:\n",
    "        \n",
    "        ans = posts['Title'][posts['Id']==ind]\n",
    "\n",
    "        ans = ans.tolist()\n",
    "\n",
    "        if len(ans)>0:\n",
    "\n",
    "            soup = BeautifulSoup(ans[0], 'html.parser')\n",
    "\n",
    "            article = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', soup.text, flags=re.MULTILINE)\n",
    "\n",
    "            article = article.replace('\\n','')\n",
    "\n",
    "            trg_scrapped.append(article)\n",
    "        \n",
    "    return trg_scrapped,scrapped_que,q_ind # all answers along with the selected question\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_measure = []\n",
    "post_col = []\n",
    "sim_col = []\n",
    "relpost_col = []\n",
    "\n",
    "def onsubmit(Post):\n",
    "\n",
    "    Answers,selected_que,q_ind = relAnswer(Post) #uncomment when function is defined\n",
    "# Answers,selected_que = relAnswer('Some suggestion for career in data science or predictive modeling')\n",
    "\n",
    "    pre_p_ans = []\n",
    "    \n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "    # print(stop_words)\n",
    "    l_w_ans = []\n",
    "    list_words_sw = []\n",
    "    for answer in Answers:\n",
    "        pun = ['!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', '_', '+', '=', '{', '}', '[', ']', '|', '\\\\', \":\", \";\", '\"', '<', ',', '>', '.', '?', '/']\n",
    "        for i in pun : \n",
    "            answer = answer.replace(i,' ')\n",
    "\n",
    "        answer = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', answer, flags=re.MULTILINE)\n",
    "    #     answer = re.sub(r'[a-z]*[A-Z]*[:.]+\\S+','',answer,flags=re.MULTILINE)\n",
    "        answer =answer.lower()\n",
    "        answer =answer.replace('i\\'m','i am')\n",
    "        answer =answer.replace(\"'\",' ')\n",
    "        answer =answer.translate(str.maketrans(' ', ' ', string.punctuation))\n",
    "\n",
    "        pre_p_ans.append(answer)\n",
    "        list_words = TreebankWordTokenizer().tokenize(answer)\n",
    "\n",
    "        for word in list_words:\n",
    "            tmp = wnl.lemmatize(word)\n",
    "            tmp = stm.stem(word)\n",
    "            l_w_ans.append(tmp)\n",
    "\n",
    "        list_words_sw.append([words for words in l_w_ans if words not in stop_words])\n",
    "\n",
    "    # TF_IDF - similarity measure for posts\n",
    "\n",
    "    pre_p_ans.append(selected_que.lower())\n",
    "    vect = TfidfVectorizer(min_df=1, stop_words=\"english\")                                                                                                                                                                                                   \n",
    "    tfidf = vect.fit_transform(pre_p_ans)                                                                                                                                                                                                                       \n",
    "    pairwise_similarity = tfidf * tfidf.T \n",
    "    sim_posts_array = pairwise_similarity.toarray()\n",
    "    for row in range(len(sim_posts_array)) :\n",
    "        for element in range(len(sim_posts_array[row])) :\n",
    "            if row == element:\n",
    "                     sim_posts_array[row][element] = 0\n",
    "\n",
    "    sim_posts_ind = sim_posts_array[-1]\n",
    "\n",
    "\n",
    "    similarity_measure.append(sim_posts_ind[:-1].tolist())\n",
    "\n",
    "#     print(similarity_measure)\n",
    "\n",
    "#     print(pairwise_similarity)\n",
    "#     print(sim_posts_ind.argmax())\n",
    "\n",
    "    # 3-columns as list for post, sim, relatedposts\n",
    "#     print(pre_p_ans)\n",
    "\n",
    "    for i in range(len(pre_p_ans)-1):\n",
    "        post_col.append(pre_p_ans[-1])\n",
    "        relpost_col.append(pre_p_ans[i])    \n",
    "        sim_col.append(similarity_measure[q_ind][i])\n",
    "        \n",
    "\n",
    "    return_string = pre_p_ans[sim_posts_ind.argmax()]\n",
    "\n",
    "    return return_string   #return the answer which is most similar to selected question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "relPost_list = []\n",
    "for post in src_scrapped:\n",
    "    relPost_list.append(onsubmit(post))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv of post with most similar related post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "relPost_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "relPost_df['Posts'] = src_scrapped\n",
    "relPost_df['RelatedPost'] = relPost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "relPost_df.to_csv('SimilarPosts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_post_ids = pd.DataFrame()\n",
    "sim_post_ids['postid'] = src_nodes_u\n",
    "sim_post_ids['postid_rp'] = target_nodes\n",
    "sim_post_ids['most_sim'] = \n",
    "sim_post_ids.to_csv('SimPost_ids.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv triple store format (post- similarity - related post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsf_post = pd.DataFrame()\n",
    "tsf_post['Post'] = post_col\n",
    "tsf_post['Similarity Measure'] = sim_col\n",
    "tsf_post['Related Posts'] = relpost_col\n",
    "tsf_post.to_csv('SimilarityMeasure_TripleStore.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#knowledge graph out of tsf_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Relationship, Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph(uri=\"http://localhost:7474\",user=\"neo4j\",password=\"1234\")\n",
    "g.delete_all\n",
    "kg = g.begin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for ind in range(len(tsf_post)):\n",
    "    \n",
    "    p = Node('Post', title = tsf_post['Post'][ind])\n",
    "    kg.merge(p,'Post', 'title')\n",
    "    \n",
    "    rp = Node('Post',title = tsf_post['Related Posts'][ind])\n",
    "    kg.merge(rp, 'Post','title')\n",
    "    \n",
    "    sim = Relationship(p , str(tsf_post['Similarity Measure'][ind]) , rp)\n",
    "    kg.create(sim)\n",
    "    \n",
    "kg.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
